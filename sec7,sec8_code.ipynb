{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# section 7\n",
    "# analogy task\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "info = api.info()  # show info about available models/datasets\n",
    "corpus = api.load('text8')\n",
    "model1 = Word2Vec(corpus)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(datapath(\"C:/naheas/classes/NLP_CS5740/project/NLP-HW/google_wordvec/GoogleNews-vectors-negative300.bin\"), binary=True)\n",
    "# we can change models between google news and text8\n",
    "# depending on use of model or model1\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "file_path=\"C:/naheas/classes/NLP_CS5740/project/analogy_test.txt\"\n",
    "Fstream = open(file_path,'r')\n",
    "Speech = Fstream.read()\n",
    "lines = Speech.split('\\n')\n",
    "ans=0.0\n",
    "not_ans=0\n",
    "for i in lines:\n",
    "    o=i.split()\n",
    "    try:\n",
    "        output=model.most_similar(positive=[o[2], o[1]], negative=[o[0]])\n",
    "        if o[3]==(output[0])[0]:\n",
    "            ans=ans+1\n",
    "        else:\n",
    "            print str(o) + \" \" + (output[0])[0]\n",
    "            not_ans=not_ans+1\n",
    "    except:\n",
    "        not_ans=not_ans+1\n",
    "print (ans/len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#section 7\n",
    "# new analogy task\n",
    "print model.doesnt_match('mexican salsa nachos wireless'.split())\n",
    "print model.doesnt_match('korean television kimchi seoul'.split())\n",
    "print model.doesnt_match('car truck bus driver'.split())\n",
    "\n",
    "oo = 'work working act acting'.split()\n",
    "print str(oo) + \" prediction: \" + str(model.most_similar(positive=[oo[2], oo[1]], negative=[oo[0]])[0])\n",
    " \n",
    "oo = 'do doing have having'.split()\n",
    "print str(oo) + \" prediction: \" + str(model.most_similar(positive=[oo[2], oo[1]], negative=[oo[0]])[0])\n",
    "\n",
    "oo = 'sleep sleeping go going'.split()\n",
    "print str(oo) + \" prediction: \" + str(model.most_similar(positive=[oo[2], oo[1]], negative=[oo[0]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# section 7\n",
    "# analogy task 3\n",
    "o = model.most_similar('increase')\n",
    "for i in o:\n",
    "    print str((i)[0]) + \" \" + str((i)[1])\n",
    "o = model.most_similar('enter')\n",
    "for i in o:\n",
    "    print str((i)[0]) + \" \" + str((i)[1])\n",
    "o = model.most_similar('gain')\n",
    "for i in o:\n",
    "    print str((i)[0]) + \" \" + str((i)[1])\n",
    "o = model.most_similar('make')\n",
    "for i in o:\n",
    "    print str((i)[0]) + \" \" + str((i)[1])\n",
    "o = model.most_similar('give')\n",
    "for i in o:\n",
    "    print str((i)[0]) + \" \" + str((i)[1])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#section 8\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import NGram\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity as dist\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(datapath(\"C:/naheas/classes/NLP_CS5740/project/NLP-HW/google_wordvec/GoogleNews-vectors-negative300.bin\"), binary=True)\n",
    "\n",
    "model1=model\n",
    "\n",
    "def Get_Vector(Paragraph):\n",
    "    '''\n",
    "    Input a paragraph, return log probability calculated on given Bigram\n",
    "    '''\n",
    "    p_vec = np.zeros([300,])\n",
    "    cnt = 1\n",
    "    for token in Paragraph:\n",
    "        if token in model1:\n",
    "            cnt += 1\n",
    "            p_vec += model1[token]\n",
    "    return p_vec/float(cnt)\n",
    "\n",
    "def Predict_Ngram(Inpath = \"../test/test.txt\", Outpath = \"../Output/8_1.csv\", Train_Trump = \"../train/trump.txt\", Train_Obama = \"../train/obama.txt\"):\n",
    "    '''\n",
    "    Input:\n",
    "        Inpath  : file path for test data\n",
    "        Outpath : file path for output csv file\n",
    "        Train_Trump : file path to train Trump's bigram model\n",
    "        Train_Obama : file path to train Obama's bigram model\n",
    "    Output:\n",
    "        Return None, Output should go straight to .csv file\n",
    "    '''\n",
    "    f = open(Outpath, 'w')\n",
    "    f.write('Id,Prediction\\n')\n",
    "    #Preprocess the test set\n",
    "    Paragraphs_Trump = NGram.corpora_preprocess(Train_Trump)\n",
    "    P_Vecs_Trump = [Get_Vector(p) for p in Paragraphs_Trump]\n",
    "    Paragraphs_Obama = NGram.corpora_preprocess(Train_Obama)\n",
    "    P_Vecs_Obama = [Get_Vector(p) for p in Paragraphs_Obama]\n",
    "    Paragraphs_test = NGram.corpora_preprocess(Inpath)\n",
    "    P_Vecs_test = [Get_Vector(p) for p in Paragraphs_test]\n",
    "    for idx, pvec in enumerate(P_Vecs_test):\n",
    "        max_cosine = -50\n",
    "        isTrump = True\n",
    "        for pv_trump in P_Vecs_Trump:\n",
    "            max_cosine = max(max_cosine, dist([pvec], [pv_trump]))\n",
    "        for pv_obama in P_Vecs_Obama:\n",
    "            if dist([pvec], [pv_obama])>max_cosine:\n",
    "                isTrump = False\n",
    "                break\n",
    "        f.write(str(idx)+',')\n",
    "        if isTrump:\n",
    "            f.write('1')\n",
    "        else:\n",
    "            f.write('0')\n",
    "        f.write('\\n')\n",
    "\n",
    "def Preprocess(Train_Trump = \"../train/trump.txt\", Train_Obama = \"../train/obama.txt\"):\n",
    "    Paragraphs_Trump = NGram.corpora_preprocess(Train_Trump)\n",
    "    P_Vecs_Trump = np.array([Get_Vector(p) for p in Paragraphs_Trump[:-1]])\n",
    "    Paragraphs_Obama = NGram.corpora_preprocess(Train_Obama)\n",
    "    P_Vecs_Obama = np.array([Get_Vector(p) for p in Paragraphs_Obama[:-1]])\n",
    "    X = np.concatenate((P_Vecs_Trump,P_Vecs_Obama))\n",
    "    Y = np.concatenate((np.ones([3000,]),np.zeros([3000,])))\n",
    "    return X,Y\n",
    "\n",
    "def PreprocessFullTrain(Train_Trump = \"../full_train/trump.txt\", Train_Obama = \"../full_train/obama.txt\"):\n",
    "    Paragraphs_Trump = NGram.corpora_preprocess(Train_Trump)\n",
    "    P_Vecs_Trump = np.array([Get_Vector(p) for p in Paragraphs_Trump[:-1]])\n",
    "    Paragraphs_Obama = NGram.corpora_preprocess(Train_Obama)\n",
    "    P_Vecs_Obama = np.array([Get_Vector(p) for p in Paragraphs_Obama[:-1]])\n",
    "    X = np.concatenate((P_Vecs_Trump,P_Vecs_Obama))\n",
    "    Y = np.concatenate((np.ones([3100,]),np.zeros([3100,])))\n",
    "    return X,Y\n",
    "\n",
    "def Train_SGD(Outpath = '../Output/sgd.csv'):\n",
    "    clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\")\n",
    "    Paragraphs_test = NGram.corpora_preprocess(\"../test/test.txt\")\n",
    "    P_Vecs_test = np.array([Get_Vector(p) for p in Paragraphs_test[:-1]])\n",
    "    X,Y = PreprocessFullTrain()\n",
    "    clf.fit(X,Y)\n",
    "    results = clf.predict(P_Vecs_test)\n",
    "    Write_Result(Outpath, results)\n",
    "    \n",
    "def Train_SGDValidation(Outpath = '../Output/sgd.csv'):\n",
    "    clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\")\n",
    "    Paragraphs_test = NGram.corpora_preprocess(\"../development/obama.txt\")\n",
    "    P_Vecs_test = np.array([Get_Vector(p) for p in Paragraphs_test[:-1]])\n",
    "    X,Y = Preprocess()\n",
    "    clf.fit(X,Y)\n",
    "    results = clf.predict(P_Vecs_test)\n",
    "    \n",
    "    cnt_obama = 0;\n",
    "    for idx,label in enumerate(results):\n",
    "        if label == 0:\n",
    "            cnt_obama += 1\n",
    "    print cnt_obama\n",
    "    \n",
    "    clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\")\n",
    "    Paragraphs_test = NGram.corpora_preprocess(\"../development/trump.txt\")\n",
    "    P_Vecs_test = np.array([Get_Vector(p) for p in Paragraphs_test[:-1]])\n",
    "    X,Y = Preprocess()\n",
    "    clf.fit(X,Y)\n",
    "    results = clf.predict(P_Vecs_test)\n",
    "    \n",
    "    cnt_trump = 0;\n",
    "    for idx,label in enumerate(results):\n",
    "        if label == 1:\n",
    "            cnt_trump += 1\n",
    "    print cnt_trump\n",
    "    \n",
    "    print str((cnt_obama + cnt_trump) / float(2))\n",
    "    \n",
    "\n",
    "def Train_RandomForest(Outpath = '../Output/forest2.csv', dp = 50, n = 100):\n",
    "    clf = RandomForestClassifier(max_depth=dp, random_state=0, n_estimators=n, criterion='gini')\n",
    "    Paragraphs_test = NGram.corpora_preprocess(\"../test/test.txt\")\n",
    "    P_Vecs_test = np.array([Get_Vector(p) for p in Paragraphs_test[:-1]])\n",
    "    X,Y = PreprocessFullTrain()\n",
    "    clf.fit(X,Y)\n",
    "    results = clf.predict(P_Vecs_test)    \n",
    "    Write_Result(Outpath, results)\n",
    "    \n",
    "\n",
    "def Train_RandomForestValidation(Outpath = '../Output/forest2.csv', dp = 50, n = 100):\n",
    "    clf = RandomForestClassifier(max_depth=dp, random_state=0, n_estimators=n, criterion='gini')\n",
    "    Paragraphs_test = NGram.corpora_preprocess(\"../development/obama.txt\")\n",
    "    P_Vecs_test = np.array([Get_Vector(p) for p in Paragraphs_test[:-1]])\n",
    "    X,Y = Preprocess()\n",
    "    clf.fit(X,Y)\n",
    "    results = clf.predict(P_Vecs_test)\n",
    "    \n",
    "    cnt_obama = 0;\n",
    "    for idx,label in enumerate(results):\n",
    "        if label == 0:\n",
    "            cnt_obama += 1\n",
    "    print cnt_obama\n",
    "    \n",
    "    clf = RandomForestClassifier(max_depth=dp, random_state=0, n_estimators=n, criterion='gini')\n",
    "    Paragraphs_test = NGram.corpora_preprocess(\"../development/trump.txt\")\n",
    "    P_Vecs_test = np.array([Get_Vector(p) for p in Paragraphs_test[:-1]])\n",
    "    X,Y = Preprocess()\n",
    "    clf.fit(X,Y)\n",
    "    results = clf.predict(P_Vecs_test)\n",
    "    \n",
    "    cnt_trump = 0;\n",
    "    for idx,label in enumerate(results):\n",
    "        if label == 1:\n",
    "            cnt_trump += 1\n",
    "    print cnt_trump\n",
    "    \n",
    "    print str((cnt_obama + cnt_trump) / float(2))\n",
    "\n",
    "def Write_Result(Outpath, results):\n",
    "    f = open(Outpath, 'w')\n",
    "    f.write('Id,Prediction\\n')\n",
    "    for idx,label in enumerate(results):\n",
    "        f.write(str(idx)+',')\n",
    "        if label == 1:\n",
    "            f.write('1')\n",
    "        else:\n",
    "            f.write('0')\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "84\n",
      "86.5\n"
     ]
    }
   ],
   "source": [
    "Train_RandomForestValidation(dp=500,n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_RandomForest(dp=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "88\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "Train_SGDValidation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
